{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "3NAyKlyGxHT9"
   },
   "source": [
    "# PyTorch Tutorial - Neural Networks\n",
    "Prof. Dorien Herremans, with many thanks to Nelson Lui for the base text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "oZnM_yfCBz6P"
   },
   "source": [
    "**To edit the notebook**:\n",
    "\n",
    "There are two ways to edit the notebook.\n",
    "\n",
    "You can either open it in the \"playground\", where you can change and run cells. After closing the tab, your changes will be lost. To do so, press \"File\" > \"Open in playground\".\n",
    "\n",
    "Alternatively, you can make a copy of this notebook to your own Google Drive account through \"File\" > \"Save a copy in Drive...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "kK-sJIRG6BLp"
   },
   "source": [
    "**Activating the GPU on Colab**:\n",
    "\n",
    "Colab now gives you 12 hours of free GPU time (before you have to request a new node).\n",
    "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "mgs23T03xBK7"
   },
   "source": [
    "# Setting up the notebook on colab\n",
    "\n",
    "Let's check if we are using the GPU environment and cuda is installed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "kig3C9d9D-kA",
    "outputId": "f9177031-b958-4127-ae64-97bb3405a7fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:\n",
      "1.5.0+cu101\n",
      "GPU Detected:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch and other libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"PyTorch version:\")\n",
    "print(torch.__version__)\n",
    "print(\"GPU Detected:\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "#defining a shortcut function for later:\n",
    "import os\n",
    "using_GPU = os.path.exists('/opt/bin/nvidia-smi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "IE8RE8n5cUmJ"
   },
   "source": [
    "# Computation Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "I00992FzWysI"
   },
   "source": [
    "A computation graph is simply a way to define a sequence of operations to go from input to model output. \n",
    "\n",
    "You can think of the nodes in the graph as representing operations, and the edges in the graph represent tensors going in and out.\n",
    "\n",
    "For example, say we wanted to build a linear regression model. This has the form $\\hat y = Wx + b$. \n",
    "\n",
    "In this equation, $x$ is our input, $W$ is a learned weight matrix, $b$ is a learned bias, and $\\hat y$ is the predicted output. \n",
    "\n",
    "As a computation graph, this looks like:\n",
    "\n",
    "![Linear Regression Computation Graph](https://imgur.com/IcBhTjS.png)\n",
    "\n",
    "When implementing deep learning models, you're basically designing and specifying computation graphs. It's a bit like playing with Legos in that you're stringing together a bunch of blocks (the operations) to achieve a final desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "5d2nO7nFW6vj"
   },
   "source": [
    "# The building blocks of deep learning models\n",
    "\n",
    "`torch.nn` makes it easy to build neural nets by providing functions for specifying arbitrary computation graphs and abstractions for putting them all together. We'll start by covering a few classes in the `torch.nn` module that form basic building blocks of many deep learning applications.\n",
    "\n",
    "The classes below are all callable, so you can use them with `outputs = YourDeepLearningBlock(its_inputs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "_jzL24p4YHeT"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4UAYmVadXZzM"
   },
   "source": [
    "## Linear Layers (Affine Transforms)\n",
    "\n",
    "A linear layer (also known as an affine transform) defines a function:\n",
    "\n",
    "$$f(x) = Wx + b$$\n",
    "\n",
    "This linear transform is a core part of deep learning. $W$ and $b$ are the parameters of this layer, where $W$ is a learned weight matrix and $b$ is a learned bias vector.\n",
    "\n",
    "`nn.Linear()` takes two construction parameters: the dimensionality of the input and the dimensionality of the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vGzhJV4rYEI6",
    "outputId": "1b651d3e-9759-497d-f998-12903cc95518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7592, -0.3534, -0.4620],\n",
      "        [-0.5454,  0.1177, -0.1239]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Create a Linear layer. Input should have 5 dimensions, output will have 3.\n",
    "lin = nn.Linear(5, 3)\n",
    "# Data is a matrix of shape (2, 5). Can we use the linear layer on it?\n",
    "data = torch.randn(2, 5)\n",
    "\n",
    "# Yes! Running the data matrix through the layer outputs shape (2, 3).\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "qK_mOB6oZWZ8",
    "outputId": "3151dc1f-80d1-4602-dc7a-e2ddde68326c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5534,  1.3608, -1.0480],\n",
      "         [ 0.0909,  1.1141, -0.4942],\n",
      "         [-0.3451, -0.0370,  0.3883],\n",
      "         [-0.2696,  0.1472, -1.0272]],\n",
      "\n",
      "        [[-0.9598,  0.1807,  0.2191],\n",
      "         [-0.5421, -0.7622,  0.9238],\n",
      "         [ 0.4080,  0.3178,  0.3770],\n",
      "         [-0.5281, -0.8213, -0.0748]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# What about a matrix of shape (2, 4, 5)?\n",
    "data = torch.randn(2, 4, 5)\n",
    "# This works as well! As long as the last dimension is the specified\n",
    "# input dimension to the Linear layer, you're good.\n",
    "# Output shape: (2, 4, 3)\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "qTv_DwVnZywZ"
   },
   "outputs": [],
   "source": [
    "# But (5, 2) is an incompatible shape (uncomment and run to see error)\n",
    "data = torch.randn(5, 2)\n",
    "# print(lin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Q07tNHV4aGhw",
    "outputId": "230f37f2-9b96-4691-8bcc-4bbfc12a89a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7069, 0.7075, 0.0483],\n",
      "        [1.1795, 1.1552, 0.2072]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# But we can transpose it using t()!\n",
    "# Now its shape (2, 5) and all is fine.\n",
    "print(lin(data.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "7ObbNWGsaR60"
   },
   "source": [
    "## Nonlinearities / Activation Functions\n",
    "\n",
    "Since composing linear transformations gives you a linear transformation, we don't gain any representational power by just chaining `Linear` layers.\n",
    "\n",
    "In deep learning, we add nonlinearities after our Linear transforms, which lets us build more powerful models.\n",
    "\n",
    "PyTorch comes with a veritable zoo of nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "L0Of7Zc7bZoO",
    "outputId": "7346acf9-87aa-41d7-8104-103d009cd460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4453, -1.0604,  2.2001],\n",
      "        [ 0.7322,  0.0163, -0.4637]])\n",
      "ReLU()\n",
      "tensor([[0.0000, 0.0000, 2.2001],\n",
      "        [0.7322, 0.0163, 0.0000]])\n",
      "Tanh()\n",
      "tensor([[-0.4180, -0.7858,  0.9757],\n",
      "        [ 0.6244,  0.0163, -0.4331]])\n",
      "Sigmoid()\n",
      "tensor([[0.3905, 0.2572, 0.9003],\n",
      "        [0.6753, 0.5041, 0.3861]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2, 3)\n",
    "print(data)\n",
    "\n",
    "# Nonlinearities are layers too!\n",
    "relu = nn.ReLU()\n",
    "print(relu)\n",
    "print(relu(data))\n",
    "\n",
    "tanh = nn.Tanh()\n",
    "print(tanh)\n",
    "print(tanh(data))\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(sigmoid)\n",
    "print(sigmoid(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "P6LvhuQ5cAA0"
   },
   "source": [
    "If you'd prefer to not create a class for the nonlinearity, you can also call it functionally as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "8FC3JGzCa7nT",
    "outputId": "3ba3eea2-bc2a-4fae-a855-215f05dbcc11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1491,  0.4879,  1.2531],\n",
      "        [-0.2940,  0.7137,  0.1550]])\n",
      "ReLu:\n",
      "tensor([[0.0000, 0.4879, 1.2531],\n",
      "        [0.0000, 0.7137, 0.1550]])\n",
      "tanh:\n",
      "tensor([[-0.8175,  0.4525,  0.8492],\n",
      "        [-0.2858,  0.6130,  0.1538]])\n",
      "Sigmoid:\n",
      "tensor([[0.2406, 0.6196, 0.7778],\n",
      "        [0.4270, 0.6712, 0.5387]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2, 3)\n",
    "print(data)\n",
    "\n",
    "# Nonlinearities can also be used functionally, with no need to create a class!\n",
    "print(\"ReLu:\")\n",
    "print(torch.relu(data))\n",
    "\n",
    "print(\"tanh:\")\n",
    "print(torch.tanh(data))\n",
    "\n",
    "print(\"Sigmoid:\")\n",
    "print(torch.sigmoid(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "609lvRL3cecR"
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is used to regularize our models by randomly setting some outputs to 0. \n",
    "\n",
    "This helps to prevent overfitting by encouraging the model to look beyond specific spurious patterns and find features that generalize.\n",
    "\n",
    "**Note that we should only apply dropout during training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "62Tot0pNc8co",
    "outputId": "67b7719c-c264-48b6-bd6e-a4a969c4695d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1373, -2.3138, -1.0574],\n",
      "        [-1.5292,  0.8491,  0.7581]])\n",
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[-0.2746, -4.6277, -2.1149],\n",
      "        [-3.0584,  1.6981,  0.0000]])\n",
      "Functional dropout, training=False\n",
      "tensor([[-0.1373, -2.3138, -1.0574],\n",
      "        [-1.5292,  0.8491,  0.7581]])\n",
      "Functional dropout, training=True\n",
      "tensor([[-0.0000, -4.6277, -0.0000],\n",
      "        [-0.0000,  1.6981,  1.5162]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2, 3)\n",
    "print(data)\n",
    "\n",
    "# Create a Dropout layer and call it on input\n",
    "# Here, the probability of zeroing an element is 0.5\n",
    "dropout = nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "print(dropout(data))\n",
    "\n",
    "# Use dropout functionally, training=False by default so no change.\n",
    "print(\"Functional dropout, training=False\")\n",
    "print(F.dropout(data, 0.5, training=False))\n",
    "\n",
    "# Set training=True, so things are dropped out\n",
    "print(\"Functional dropout, training=True\")\n",
    "print(F.dropout(data, 0.5, training=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "GgQUlcmfeQum"
   },
   "source": [
    "## RNNs (LSTMs / GRUs in particular) \n",
    "\n",
    "RNNs encode a sequence of vectors as another sequence of vectors --- generally you take the final output as a representation of the input sequence. They're powerful models for sequential data, and thus they're very popular in NLP. Their API takes a tensor of shape `(sequence length, batch size, dim)` if `batch_first=False` (default) and `(batch size, sequence length, dim)` if `batch_first=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "ghk-BcDbe1XY",
    "outputId": "a743c30e-cce1-43b1-fa53-77506dae8428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "LSTM Outputs: \n",
      "tensor([[[ 0.0863,  0.0503,  0.0631,  0.2297, -0.1557],\n",
      "         [ 0.0540,  0.0459,  0.1089,  0.2475, -0.1887],\n",
      "         [ 0.1000, -0.0036,  0.0668,  0.1837, -0.3326]],\n",
      "\n",
      "        [[-0.2331, -0.3497,  0.1271, -0.1133,  0.2003],\n",
      "         [-0.2794, -0.1814,  0.1167, -0.0354,  0.0295],\n",
      "         [-0.2420, -0.2313,  0.2252,  0.0250,  0.1110]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "Final LSTM Output: \n",
      "tensor([[ 0.1000, -0.0036,  0.0668,  0.1837, -0.3326],\n",
      "        [-0.2420, -0.2313,  0.2252,  0.0250,  0.1110]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "LSTM Hidden State: \n",
      "tensor([[[ 0.1000, -0.0036,  0.0668,  0.1837, -0.3326],\n",
      "         [-0.2420, -0.2313,  0.2252,  0.0250,  0.1110]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "LSTM Cell State: \n",
      "tensor([[[ 0.1529, -0.0235,  0.1296,  0.4260, -0.6174],\n",
      "         [-0.6229, -0.3603,  0.5541,  0.0329,  0.2716]]],\n",
      "       grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Some batch-first data. Batch size is 2, sequence length is 3, \n",
    "# and length of each feature vector is 4.\n",
    "data = torch.randn(2, 3, 4)\n",
    "\n",
    "input_size = 4\n",
    "hidden_dim = 5\n",
    "# Create a LSTM RNN with hidden dim of 5\n",
    "# batch_first=True since semantics of shape is\n",
    "# (batch_size, sequence_length, num_features)\n",
    "lstm = nn.LSTM(input_size, hidden_dim, batch_first=True)\n",
    "\n",
    "\n",
    "lstm_output_tuple = lstm(data)\n",
    "print(type(lstm_output_tuple))\n",
    "\n",
    "lstm_output, (lstm_hidden_state, lstm_cell_state) = lstm_output_tuple\n",
    "print(\"LSTM Outputs: \")\n",
    "print(lstm_output)\n",
    "\n",
    "print(\"Final LSTM Output: \")\n",
    "print(lstm_output[:, -1])\n",
    "\n",
    "print(\"LSTM Hidden State: \")\n",
    "print(lstm_hidden_state)\n",
    "\n",
    "print(\"LSTM Cell State: \")\n",
    "print(lstm_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "RZNnUJcrU2r2"
   },
   "source": [
    "# Structuring PyTorch models\n",
    "\n",
    "At the highest level, `nn.Module` defines what most would refer to as a \"model\". It's a convenient way for encapsulating the trainable parameters of a model or a component of your model, and subclassing this class gives you Python functions for moving your model to the GPU, saving it, loading it etc.\n",
    "\n",
    "When you're building your own model, you're going to subclass `nn.Module`. Critically, you also need to override the `__init__()` and `forward()` functions.\n",
    "\n",
    "*   In `__init__()`, you should take arguments that modify how the model runs (e.g. # of layers, # of hidden units, output sizes). You'll also set up most of the layers that you use in the forward pass here.\n",
    "*   In `forward()`, you define the \"forward pass\" of your model, or the operations needed to transform input to output. **You can use any of the Tensor operations in the forward pass.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "goB1WdBQ3TKd"
   },
   "source": [
    "### Feed-forward neural net\n",
    "\n",
    "To improve upon the logistic regression model we saw last week, we can add some intermediate layers (called hidden layers), nonlinearities, and dropout for regularization. This is essentially a multi-layer feed forward neural net, and it's implementation as a module is outlined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "dXe9xPrn3zSK"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "  # input_size: Dimensionality of input feature vector.\n",
    "  # num_classes: The number of classes in the classification problem.\n",
    "  # num_hidden: The number of hidden (intermediate) layers to use.\n",
    "  # hidden_dim: The size of each of the hidden layers.\n",
    "  # dropout: The proportion of units to drop out after each layer.\n",
    "  def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
    "    # Always call the superclass (nn.Module) constructor first!\n",
    "    super(FeedForwardNN, self).__init__()\n",
    "    \n",
    "    # Set up the hidden layers.\n",
    "    assert num_hidden > 0\n",
    "    # A special ModuleList to store our hidden layers.\n",
    "    self.hidden_layers = nn.ModuleList([])\n",
    "    # First hidden layer maps from input_size -> num_hidden.\n",
    "    self.hidden_layers.append(nn.Linear(input_size, hidden_dim))\n",
    "    # Subsequent hidden layers map from num_hidden -> num_hidden.\n",
    "    # Note that they can map to any dimensionality --- as long as the final\n",
    "    # output is a distribution over your classes!\n",
    "    for i in range(num_hidden - 1):\n",
    "      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "    \n",
    "    # Set up the dropout layer.\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # Set up the final transform to a distribution over classes.\n",
    "    self.output_projection = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    # Set up the nonlinearity to use between layers.\n",
    "    self.nonlinearity = nn.ReLU()\n",
    "    \n",
    "  # Forward's sole argument is the input.\n",
    "  # input is of shape (batch_size, input_size)\n",
    "  def forward(self, x):\n",
    "    # Apply the hidden layers, nonlinearity, and dropout.\n",
    "    for hidden_layer in self.hidden_layers:\n",
    "      x = hidden_layer(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.nonlinearity(x)\n",
    "      \n",
    "    # Output layer: project x to a distribution over classes.\n",
    "    out = self.output_projection(x)\n",
    "    \n",
    "    # Softmax the out tensor to get a log-probability distribution\n",
    "    # over classes for each example.\n",
    "    out_distribution = F.log_softmax(out, dim=-1)\n",
    "    return out_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "c_oWVoYX6IJo"
   },
   "source": [
    "# Training PyTorch models: Losses and Optimizers\n",
    "\n",
    "By now, we've learned how to construct models in PyTorch. In this section, we'll go over how to calculate your model's loss and how to optimize the parameters to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "R-FcsSK266VJ"
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "Intuitively, loss functions serve to tell your model how poorly it's doing --- the purpose of training is to adjust the weights of our model to minimize the loss.\n",
    "\n",
    "A loss function takes a true output $y$ and a model-predicted output $\\hat y$ and calculates the loss. If $y = \\hat y$, our model produced the correct output and thus our loss is 0. The further our predicted $\\hat y$ from the true $y$, the higher our loss is.\n",
    "\n",
    "PyTorch comes with a large collection of loss functions. The most commonly used loss for classification is negative log likelihood (`nn.NLLLoss` or the very related `nn.CrossEntropyLoss`). The difference between `nn.NLLLoss` and `nn.CrossEntropyLoss` for classification problems is that `nn.NLLLoss` expects the output to be log-softmax normalized, which is easy to do with the `nn.LogSoftmax` layer. On the other hand `nn.CrossEntropyLoss`, automatically applies the log-softmax --- you can think of it as `nn.LogSoftmax` + `nn.NLLLoss`. Which to use depends on whether you want to add the extra `nn.LogSoftmax` to your model's `forward()`.\n",
    "\n",
    "A common loss used for regression problems is the mean squared error (`nn.MSELoss`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "VfdPEetZARPk"
   },
   "source": [
    "Here's a usage example of the `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "jHP6WENd9ql4",
    "outputId": "af6e5a98-b64e-4f00-f8d0-f23d16f81043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss averaged across all 3 batch elements:\n",
      "tensor(1.4539, grad_fn=<NllLossBackward>)\n",
      "Gradients of model_output\n",
      "tensor([[ 0.1060, -0.2505,  0.0481,  0.0963],\n",
      "        [-0.2536,  0.1149,  0.0930,  0.0458],\n",
      "        [ 0.0849,  0.0889,  0.0879, -0.2618]])\n"
     ]
    }
   ],
   "source": [
    "# 3 examples, unnormalized scores over 4 classes.\n",
    "model_output = torch.rand(3, 4, requires_grad = True)\n",
    "\n",
    "# The correct labels.\n",
    "targets = torch.LongTensor([1, 0, 3])\n",
    "\n",
    "# CrossEntropyLoss\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "# Loss, averaged across all 3 batch elements.\n",
    "# Can call this functionally: avg_loss = F.cross_entropy(model_output, targets)\n",
    "avg_loss = cross_entropy(model_output, targets)\n",
    "print(\"CrossEntropyLoss averaged across all 3 batch elements:\")\n",
    "print(avg_loss)\n",
    "\n",
    "# Backpropagate wrt avg_loss\n",
    "avg_loss.backward()\n",
    "# Print out the gradients of model_output\n",
    "print(\"Gradients of model_output\")\n",
    "print(model_output.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "VALpI3evA2H-"
   },
   "source": [
    "And here's a snippet showing that `LogSoftmax` + `NLLLoss` is the same as `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lLB3icyzA1bv",
    "outputId": "46b41dc3-ad00-4b41-8203-f8176299115b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative-Log Likelihood averaged across all 3 batch elements:\n",
      "tensor(1.4539, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "nll = nn.NLLLoss()\n",
    "log_softmax_model_output = F.log_softmax(model_output, dim=-1)\n",
    "# Loss, averaged across all 3 batch elements.\n",
    "# Can call this functionally: avg_loss = F.nll_loss(model_output, targets)\n",
    "avg_loss = nll(log_softmax_model_output, targets)\n",
    "print(\"Negative-Log Likelihood averaged across all 3 batch elements:\")\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "6C4np6TBEToZ"
   },
   "source": [
    "## Optimizers\n",
    "\n",
    "Now that we can calculate the loss and backpropagate through our model (with `.backward()`), we can update the weights and try to reduce the loss!\n",
    "\n",
    "PyTorch includes a variety of optimizers that do exactly this, from the standard SGD to more recent techniques like Adam and RMSProp.\n",
    "\n",
    "At construction, PyTorch parameters take the parameters to optimize. When we run an input through our model, calculate the loss, and backpropagate, the gradients are automatically stored in the parameters (since they're all `Variables`). With these gradients, the optimizer can update the weights.\n",
    "\n",
    "Optimizers live in the `torch.optim` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "FSWQMLFIPqNt"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "zMNBP65mKvNY"
   },
   "source": [
    "To get the parameters of our model, we can just call `.parameters()` on a `Module`. Below, we create an instance of our previously-defined feed forward neural network and get its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "e0s6-Y6LKurn",
    "outputId": "ab4b53bd-618a-4c22-99b2-66f7e7a827ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNN(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_projection): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (nonlinearity): ReLU()\n",
      ")\n",
      "Shapes of model parameters:\n",
      "[torch.Size([50, 784]), torch.Size([50]), torch.Size([50, 50]), torch.Size([50]), torch.Size([10, 50]), torch.Size([10])]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_hidden = 2\n",
    "hidden_dim = 50\n",
    "dropout = 0.2\n",
    "ffnn_clf = FeedForwardNN(input_size, num_classes, num_hidden, \n",
    "                         hidden_dim, dropout)\n",
    "print(ffnn_clf)\n",
    "\n",
    "parameters = ffnn_clf.parameters()\n",
    "\n",
    "print(\"Shapes of model parameters:\")\n",
    "print([x.size() for x in list(parameters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "SmnL2QaEPcu5"
   },
   "source": [
    "Now to create an optimizer for this model, we construct a optimizer class and pass it the parameters of the model: stochastic gradient descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "_eUwD2d1PizL"
   },
   "outputs": [],
   "source": [
    "ffnn_optim = optim.SGD(ffnn_clf.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "O0LwAANLQUe8"
   },
   "source": [
    "Let's try using our optimizer to take a gradient update on our model! We'll generate a few random examples, and run them through our model (the forward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "6SHk4heLQk3a",
    "outputId": "d1646671-ce81-4baf-94da-c75fe3545aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted distribution over classes: \n",
      "tensor([[-2.2224, -2.2983, -2.5725, -2.4691, -2.3502, -2.1213, -2.3924, -2.0233,\n",
      "         -2.3391, -2.3545],\n",
      "        [-2.1385, -2.2591, -2.6362, -2.1620, -2.2960, -2.2317, -2.4771, -2.1674,\n",
      "         -2.2380, -2.5529],\n",
      "        [-2.1252, -2.4054, -2.6909, -2.4490, -2.1260, -2.0059, -2.4105, -2.2823,\n",
      "         -2.3714, -2.3317],\n",
      "        [-2.2720, -2.3720, -2.4838, -2.5002, -2.0611, -2.1576, -2.4864, -2.1590,\n",
      "         -2.4618, -2.1959],\n",
      "        [-2.2308, -2.2916, -2.4355, -2.4158, -2.3571, -2.0843, -2.3079, -2.1659,\n",
      "         -2.5263, -2.2882]], grad_fn=<LogSoftmaxBackward>)\n",
      "Target Labels:\n",
      "tensor([0, 3, 9, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Make some fake data for our model.\n",
    "# 5 examples in the batch, each example has 784 features.\n",
    "sample_input = torch.randn(5, 784)\n",
    "# Multilabel classification, 10 possible classes.\n",
    "sample_labels = torch.LongTensor([0, 3, 9, 6, 2])\n",
    "\n",
    "# Run the sample_input through ffnn_clf to get a distribution\n",
    "# over our classes\n",
    "sample_predictions = ffnn_clf(sample_input)\n",
    "print(\"Predicted distribution over classes: \")\n",
    "print(sample_predictions)\n",
    "print(\"Target Labels:\")\n",
    "print(sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Ek8IxTtjS8vn"
   },
   "source": [
    "Now let's calculate the loss of our model on these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cmFz2EiBS_u7",
    "outputId": "e76d5723-39f6-4c80-e060-eafdbdbe14e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL Loss:\n",
      "tensor(2.3276, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "nll_loss = F.nll_loss(sample_predictions, sample_labels)\n",
    "print(\"Average NLL Loss:\")\n",
    "print(nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "_MVYKKL7TbiV"
   },
   "source": [
    "Let's print the gradients of one of the parameter matrices in our model, to ensure it's `None`. We haven't done backprop yet, so there shouldn't be any gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iuffHd7WTj6Z",
    "outputId": "435420f7-711a-48fc-9ac2-7fb15019a148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(list(ffnn_clf.parameters())[0].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "m-P0XKUvTRqg"
   },
   "source": [
    "Now we can backpropagate with respect to the loss to calculate the gradients for the parameters of our model with `.backward()`. It's also good practice to call `optimizer.zero_grad()` before `loss.backwards()`, which ensures that the gradients are reset to 0 before backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "tcXTmYqlTQ8h"
   },
   "outputs": [],
   "source": [
    "ffnn_optim.zero_grad()\n",
    "nll_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "UxBJRkWjTvir"
   },
   "source": [
    "Let's check our gradients now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "PAhrOjDyTxls",
    "outputId": "8eae11f7-a1dc-4a35-a528-94656ab7bc5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0120, -0.0152, -0.0066,  ...,  0.0066,  0.0079, -0.0081],\n",
      "        [ 0.0081, -0.0106, -0.0029,  ...,  0.0100,  0.0013, -0.0020],\n",
      "        [-0.0103,  0.0022,  0.0085,  ..., -0.0278,  0.0174,  0.0100],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0007, -0.0197,  ...,  0.0111,  0.0100, -0.0066],\n",
      "        [ 0.0121,  0.0036, -0.0240,  ...,  0.0056, -0.0077,  0.0019],\n",
      "        [ 0.0260, -0.0327, -0.0147,  ...,  0.0125,  0.0184, -0.0187]])\n"
     ]
    }
   ],
   "source": [
    "print(list(ffnn_clf.parameters())[0].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "r3vNshvNUELX"
   },
   "source": [
    "Now that we have gradients for each of our parameters, we can update them by using `optimizer.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "LBeoxxulUHj7",
    "outputId": "3e493ea0-140a-44da-d27e-d9563e100cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between weight matrix before and after update:\n",
      "tensor([[ 0.0060, -0.0076, -0.0033,  ...,  0.0033,  0.0039, -0.0041],\n",
      "        [ 0.0041, -0.0053, -0.0014,  ...,  0.0050,  0.0006, -0.0010],\n",
      "        [-0.0052,  0.0011,  0.0043,  ..., -0.0139,  0.0087,  0.0050],\n",
      "        ...,\n",
      "        [ 0.0030,  0.0003, -0.0099,  ...,  0.0056,  0.0050, -0.0033],\n",
      "        [ 0.0060,  0.0018, -0.0120,  ...,  0.0028, -0.0039,  0.0009],\n",
      "        [ 0.0130, -0.0163, -0.0074,  ...,  0.0063,  0.0092, -0.0094]])\n"
     ]
    }
   ],
   "source": [
    "# save the old value of the parameter for comparison later\n",
    "old_parameter = list(ffnn_clf.parameters())[0].data.clone()\n",
    "\n",
    "# Make a gradient update with our optimizer\n",
    "ffnn_optim.step()\n",
    "\n",
    "new_parameter = list(ffnn_clf.parameters())[0].data\n",
    "\n",
    "print(\"Difference between weight matrix before and after update:\")\n",
    "print(old_parameter - new_parameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "WCKZg7yUZban"
   },
   "source": [
    "If you're familiar with the SGD update rule, you know that:\n",
    "\n",
    "$$ \\theta^{t+1} = \\theta^{t} - \\left( \\eta \\cdot \\nabla L \\left(\\theta^{t} \\right) \\right)$$\n",
    "\n",
    "Where $\\theta^{t}$ is the weight at time $t$, $\\eta$ is the learning rate, $\\nabla L(\\theta^{t})$ is the gradient. Since $\\eta = 0.5$, it makes perfect sense that the difference between the weight vectors printed above is exactly half of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "r6M0pgwgcFk0"
   },
   "source": [
    "# Example: Classification on FashionMNIST\n",
    "\n",
    "Let's use the `FeedForwardNN` model we built earlier to do a simple classification task! This example is meant to be an annotated walkthrough of how to build, train, and evaluate a model in PyTorch. We'll use the [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist), where we are tasked with classifying black and white images of clothes into 10 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "3Vw-6V7XhHs3"
   },
   "source": [
    "## Loading Data\n",
    "\n",
    "We'll start by loading the data with `torchvision` --- knowing how to use torchvision isn't the point of this tutorial, so it's relatively unannotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352,
     "referenced_widgets": [
      "e786c2ca602c495d9a5cef9f8c156f9a",
      "730a7fcd528a433b8dfb0ec76d09db28",
      "a7b0091ca99b4a0493855c50a5f6d3b1",
      "41f12ed008d44ccfab604edf77ce97fd",
      "a9626a1df05741a0b8929bc590b4f310",
      "51e80c76578c400ab4d197ea6344c437",
      "5c849e15cb674970ba757ee7ef7f2d34",
      "54a601dbf01e44dc9243d8dddf293626",
      "3b87ae347817466abae90da93bb18cc8",
      "4612536ae2bc405789cd111b81095c20",
      "b9df8d475e6b4541a04d1fcaadd21cea",
      "60ec53e0872345a993ebfe93a79eb6ef",
      "51c9865f5c364560861542eca2a0fb1a",
      "6b190e8d15914abb83564c6838659743",
      "61817e09ee6644409c6f94c437c09085",
      "33972cfe3dba42f7b7cb22d1bb6f2c35",
      "063343a6ed0c4430a59ed2a43adeced7",
      "b7ad32af1be54da8b3a9bcb87a6f46c9",
      "3619f56399b04a5cae084253d490cb24",
      "6178f6d241d24e5bb80d0849592dc46f",
      "861879c943bd4adf8fa20cd29d6574a6",
      "0af4442d59474db3bdf48b8d43c5630c",
      "30b7d427ca524d5bbb540f9a29f925d2",
      "8925827917604b13bbe5739e48b064e6",
      "c4fdec40a7a84c4aa22d7a6aadc6f0b9",
      "92a296dbe5ba4293872f777ac62e279e",
      "946e06cfaa6c44218f7f2a9800587325",
      "33f292e3ed83456fb25310adfdd51eed",
      "f649c6038d9d4a6d9c60f66b11ca51bc",
      "c3b8b5473461477a9d6c838029137c4a",
      "3e283adad2754727a1a542d6c2cc3963",
      "a30628017c8c49b193fd128edd92fb16"
     ]
    },
    "colab_type": "code",
    "id": "WgbJYcQRiG2A",
    "outputId": "ca81eb6d-a80b-424a-dc74-1d0d0e8b3af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e786c2ca602c495d9a5cef9f8c156f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./torchvision-data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b87ae347817466abae90da93bb18cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./torchvision-data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063343a6ed0c4430a59ed2a43adeced7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./torchvision-data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fdec40a7a84c4aa22d7a6aadc6f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./torchvision-data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "train_dataset = FashionMNIST(root='./torchvision-data', \n",
    "                             train=True, \n",
    "                             transform=torchvision.transforms.ToTensor(),\n",
    "                             download=True)\n",
    "\n",
    "test_dataset = FashionMNIST(root='./torchvision-data', train=False, \n",
    "                            transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ry0GX49nj2T5"
   },
   "source": [
    "`train_dataset` and `test_dataset` are both subclasses of PyTorch's `torch.utils.data.Dataset`. The main benefit of subclassing this abstract class is that we can use `torch.utils.data.DataLoader`s to handle batching our examples and iterating over them. We'll create `DataLoader`s for our datasets now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "diAuxKlRn8pZ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data-related hyperparameters\n",
    "batch_size = 64\n",
    "\n",
    "# Set up a DataLoader for the training dataset.\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set up a DataLoader for the test dataset.\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "6VWx3AsQoixs"
   },
   "source": [
    "Let's take a look at what's inside our datasets. `torch.utils.data.Dataset`s are indexable, so we can easily peek inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NS2eyMtSovq9",
    "outputId": "f1f088f4-7c22-46d6-ab3a-feb3370c1dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
      "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0039, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
      "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
      "          0.0157, 0.0000, 0.0000, 0.0118],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
      "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0471, 0.0392, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
      "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
      "          0.3020, 0.5098, 0.2824, 0.0588],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
      "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
      "          0.5529, 0.3451, 0.6745, 0.2588],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
      "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
      "          0.4824, 0.7686, 0.8980, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
      "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
      "          0.8745, 0.9608, 0.6784, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
      "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
      "          0.8627, 0.9529, 0.7922, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
      "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
      "          0.8863, 0.7725, 0.8196, 0.2039],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
      "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
      "          0.9608, 0.4667, 0.6549, 0.2196],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
      "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
      "          0.8510, 0.8196, 0.3608, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
      "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
      "          0.8549, 1.0000, 0.3020, 0.0000],\n",
      "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
      "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
      "          0.8784, 0.9569, 0.6235, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
      "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
      "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
      "          0.9137, 0.9333, 0.8431, 0.0000],\n",
      "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
      "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
      "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
      "          0.8627, 0.9098, 0.9647, 0.0000],\n",
      "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
      "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
      "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
      "          0.8706, 0.8941, 0.8824, 0.0000],\n",
      "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
      "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
      "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
      "          0.8745, 0.8784, 0.8980, 0.1137],\n",
      "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
      "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
      "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
      "          0.8627, 0.8667, 0.9020, 0.2627],\n",
      "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
      "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
      "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
      "          0.7098, 0.8039, 0.8078, 0.4510],\n",
      "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
      "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
      "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
      "          0.6549, 0.6941, 0.8235, 0.3608],\n",
      "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
      "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
      "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
      "          0.7529, 0.8471, 0.6667, 0.0000],\n",
      "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
      "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
      "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
      "          0.3882, 0.2275, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
      "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 9)\n"
     ]
    }
   ],
   "source": [
    "# Print the first training example\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "_WHfersco6pG"
   },
   "source": [
    "From this output, we can see the dataset elements are tuple of `(data_tensor, label)`. `data_tensor` is a `FloatTensor` of shape `(1, 28, 28)` (since the image is 28x28), and `label` is an integer from 0 to 9 (since there are 10 classes in the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "qlass4LfpKyL"
   },
   "source": [
    "Let's similarly look at what the `DataLoader` produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fYxZWUHFpQCD",
    "outputId": "a2afc876-b279-4dff-cdb3-ed6fbf103874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.1804, 0.1529, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]),\n",
       " tensor([3, 6, 1, 2, 6, 4, 7, 7, 1, 9, 5, 5, 5, 5, 8, 4, 5, 6, 6, 3, 0, 2, 4, 7,\n",
       "         0, 7, 7, 8, 6, 1, 8, 5, 4, 8, 4, 8, 1, 7, 3, 9, 2, 6, 9, 6, 4, 1, 0, 2,\n",
       "         4, 0, 3, 4, 2, 0, 1, 6, 9, 4, 5, 3, 4, 6, 9, 9])]"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataloader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "kj9suOk1ppR7"
   },
   "source": [
    "As we can see, the `DataLoader` groups examples into batches of size `batch_size` (64 by default in the code above). Thus, the shape of the returned tensor is `(64, 1, 28, 28)`, since we essentially stacked `batch_size` examples together. Similarly, `labels` is now a `LongTensor` of size `batch_size`. \n",
    "\n",
    "Note that the label for a single example was a Python `int` --- the dataloader automatically grouped them into a `LongTensor` of the appropriate size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "kOsLTW_hkneW"
   },
   "source": [
    "## Building our model\n",
    "\n",
    "Now we can construct a `FeedForwardNN` instance that we'll train. Each FashionMNIST example is `28x28`, so we get it as a Tensor of shape `(28, 28)`.\n",
    "\n",
    "We'll flatten out each example to a vector of size `(784,)` for compatibility with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "QVfwXTtdlCH1",
    "outputId": "789dfde4-6fdd-447e-eb59-775ae0ccbaf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNN(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_projection): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (nonlinearity): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters of our model.\n",
    "num_hidden = 2\n",
    "hidden_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "fashionmnist_ffnn_clf = FeedForwardNN(input_size=784, num_classes=10, \n",
    "                                      num_hidden=num_hidden, \n",
    "                                      hidden_dim=hidden_dim, dropout=dropout)\n",
    "print(fashionmnist_ffnn_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "KufdJjQllZ9r"
   },
   "source": [
    "If we're using a GPU, we'll move the model to the GPU which should speed up training. We do this with the same `.cuda()` method we used for Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PDzmZsi3lmea",
    "outputId": "8a45c975-871e-4d5f-874e-af376aa7e2b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU?:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "if using_GPU:\n",
    "  fashionmnist_ffnn_clf = fashionmnist_ffnn_clf.cuda()\n",
    "\n",
    "# Check if the Module is on GPU by checking if a parameter is on GPU\n",
    "print(\"Model on GPU?:\")\n",
    "print(next(fashionmnist_ffnn_clf.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "v-zvR2a7mNBf"
   },
   "source": [
    "## Construct other classes we need for training: loss and optimizer\n",
    "\n",
    "Now, we'll set up a criterion for calculating the loss and an Optimizer for updating our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "QEIHPun3mapF"
   },
   "outputs": [],
   "source": [
    "# Set up criterion for calculating loss\n",
    "nll_criterion = nn.NLLLoss()\n",
    "\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
    "ffnn_optimizer = optim.SGD(fashionmnist_ffnn_clf.parameters(), \n",
    "                           lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "fLpxMEcPnBSS"
   },
   "source": [
    "## Train the model!\n",
    "\n",
    "Now, we'll implement the procedure to train the model --- this is typically called the \"train loop\" since we loop over our batches, performing the forward pass, calculating a loss, backpropping, and then updating our parameters. This is the bulk of the code necessary to train the model.\n",
    "\n",
    "This block looks pretty long, but that's mostly because of the comments :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "jFWe_FxbnTTm",
    "outputId": "9ab4ab12-28e7-4032-e850-a2b2bd71f058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500. Test Loss 0.5436351299285889. Test Accuracy 80.\n",
      "Starting epoch 2\n",
      "Iteration 1000. Test Loss 0.5949483513832092. Test Accuracy 77.\n",
      "Iteration 1500. Test Loss 0.582830011844635. Test Accuracy 78.\n",
      "Starting epoch 3\n",
      "Iteration 2000. Test Loss 0.5791431069374084. Test Accuracy 79.\n",
      "Iteration 2500. Test Loss 0.5144996643066406. Test Accuracy 81.\n",
      "Starting epoch 4\n",
      "Iteration 3000. Test Loss 0.48827216029167175. Test Accuracy 83.\n",
      "Iteration 3500. Test Loss 0.5154128670692444. Test Accuracy 82.\n",
      "Starting epoch 5\n",
      "Iteration 4000. Test Loss 0.5071067214012146. Test Accuracy 82.\n",
      "Iteration 4500. Test Loss 0.46210917830467224. Test Accuracy 84.\n",
      "Starting epoch 6\n",
      "Iteration 5000. Test Loss 0.497216135263443. Test Accuracy 82.\n",
      "Iteration 5500. Test Loss 0.4662243127822876. Test Accuracy 83.\n",
      "Starting epoch 7\n",
      "Iteration 6000. Test Loss 0.4709796607494354. Test Accuracy 83.\n",
      "Iteration 6500. Test Loss 0.46912527084350586. Test Accuracy 83.\n",
      "Starting epoch 8\n",
      "Iteration 7000. Test Loss 0.4808599054813385. Test Accuracy 81.\n",
      "Iteration 7500. Test Loss 0.5075136423110962. Test Accuracy 80.\n",
      "Starting epoch 9\n",
      "Iteration 8000. Test Loss 0.46308353543281555. Test Accuracy 83.\n",
      "Starting epoch 10\n",
      "Iteration 8500. Test Loss 0.46827563643455505. Test Accuracy 83.\n",
      "Iteration 9000. Test Loss 0.4630490243434906. Test Accuracy 83.\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs (passes through the dataset) to train the model for.\n",
    "num_epochs = 10\n",
    "\n",
    "# A counter for the number of gradient updates we've performed.\n",
    "num_iter = 0\n",
    "\n",
    "# Iterate `num_epochs` times.\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"Starting epoch {}\".format(epoch + 1))\n",
    "  # Iterate over the train_dataloader, unpacking the images and labels\n",
    "  for (images, labels) in train_dataloader:\n",
    "    # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784), since\n",
    "    # that's what our model expects. Remember that -1 does shape inference!\n",
    "    reshaped_images = images.view(-1, 784)\n",
    "    \n",
    "    # Wrap reshaped_images and labels in Variables,\n",
    "    # since we want to calculate gradients and backprop.\n",
    "    reshaped_images = Variable(reshaped_images)\n",
    "    labels = Variable(labels)\n",
    "    \n",
    "    # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
    "    if using_GPU:\n",
    "      reshaped_images = reshaped_images.cuda()\n",
    "      labels = labels.cuda()\n",
    "      \n",
    "    # Run the forward pass through the model to get predicted log distribution.\n",
    "    # predicted shape: (batch_size, 10) (since there are 10 classes)\n",
    "    predicted = fashionmnist_ffnn_clf(reshaped_images)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    batch_loss = nll_criterion(predicted, labels)\n",
    "    \n",
    "    # Clear the gradients as we prepare to backprop.\n",
    "    ffnn_optimizer.zero_grad()\n",
    "    \n",
    "    # Backprop (backward pass), which calculates gradients.\n",
    "    batch_loss.backward()\n",
    "    \n",
    "    # Take a gradient step to update parameters.\n",
    "    ffnn_optimizer.step()\n",
    "    \n",
    "    # Increment gradient update counter.\n",
    "    num_iter += 1\n",
    "    \n",
    "    # Calculate test set loss and accuracy every 500 gradient updates\n",
    "    # It's standard to have this as a separate evaluate function, but\n",
    "    # we'll place it inline for didactic purposes.\n",
    "    if num_iter % 500 == 0:\n",
    "      # Set model to eval mode, which turns off dropout.\n",
    "      fashionmnist_ffnn_clf.eval()\n",
    "      # Counters for the num of examples we get right / total num of examples.\n",
    "      num_correct = 0\n",
    "      total_examples = 0\n",
    "      total_test_loss = 0\n",
    "      \n",
    "      # Iterate over the test dataloader\n",
    "      for (test_images, test_labels) in test_dataloader:\n",
    "        # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784) again\n",
    "        reshaped_test_images = test_images.view(-1, 784)\n",
    "    \n",
    "        # Wrap test data in Variable, like we did earlier.\n",
    "        # We set volatile=True bc we don't need history; speeds up inference.\n",
    "        reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
    "        test_labels = Variable(test_labels, volatile=True)\n",
    "    \n",
    "        # If we're using the GPU, move tensors to the GPU.\n",
    "        if using_GPU:\n",
    "          reshaped_test_images = reshaped_test_images.cuda()\n",
    "          test_labels = test_labels.cuda()\n",
    "          \n",
    "        # Run the forward pass to get predicted distribution.\n",
    "        predicted = fashionmnist_ffnn_clf(reshaped_test_images)\n",
    "        \n",
    "        # Calculate loss for this test batch. This is averaged, so multiply\n",
    "        # by the number of examples in batch to get a total.\n",
    "        total_test_loss += nll_criterion(\n",
    "            predicted, test_labels).data * test_labels.size(0)\n",
    "        \n",
    "        # Get predicted labels (argmax)\n",
    "        # We need predicted.data since predicted is a Variable, and torch.max\n",
    "        # expects a Tensor as input. .data extracts Tensor underlying Variable.\n",
    "        _, predicted_labels = torch.max(predicted.data, 1)\n",
    "        \n",
    "        # Count the number of examples in this batch\n",
    "        total_examples += test_labels.size(0)\n",
    "        \n",
    "        # Count the total number of correctly predicted labels.\n",
    "        # predicted == labels generates a ByteTensor in indices where\n",
    "        # predicted and labels match, so we can sum to get the num correct.\n",
    "        num_correct += torch.sum(predicted_labels == test_labels.data)\n",
    "      accuracy = 100 * num_correct / total_examples\n",
    "      average_test_loss = total_test_loss / total_examples\n",
    "      print(\"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
    "          num_iter, average_test_loss, accuracy))\n",
    "      # Set the model back to train mode, which activates dropout again.\n",
    "      fashionmnist_ffnn_clf.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "5XCqzzlc3_Zw"
   },
   "source": [
    "## Extending this example\n",
    "\n",
    "A good exercise to extend this example would be to look in to how convolutional neural nets (CNNs) work, build a `ConvolutionalNeuralNet` classification `Module`, train it on the FashionMNIST dataset, and compare the feed-forward neural net with the CNN. We explore CNNs next week. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Q-3l-5R9yw1Q"
   },
   "source": [
    "# Example: Sentiment classification with RNN\n",
    "\n",
    "More examples and details at the source + base text: https://github.com/bentrevett/pytorch-sentiment-analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "bYyDaX4_zIcZ"
   },
   "source": [
    "The task tackled here is sentiment analysis of IMDB articles. In this notebook, we'll actually get decent results. We will take the first base model from the url above and build upon it so it becomes more efficient. Some of the concepts we will use: \n",
    "\n",
    "We will use:\n",
    "- packed padded sequences\n",
    "- pre-trained word embeddings\n",
    "- a type of RNN architecture\n",
    "- bidirectional RNN\n",
    "- multi-layer RNN\n",
    "- regularization\n",
    "- Adam optimizer\n",
    "\n",
    "The general architecture will be: pretrained word embeddings (to capture meaning of the words), fed into an LSTM which captures longer term time dependencies of sentences, to predict the sentiment. \n",
    "\n",
    "This will allow us to achieve ~84% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "o9nEkjrSzIca"
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "We will set a seed first. Why do we do this? Because then our random function is initialized in the same way and we all get the same 'random' results. Then we will define the `Fields` and get the train/valid/test splits.\n",
    "\n",
    "Since our sentences are all of a different length, we typically 'pad' them, so they all become of the same length. We take it one step further however, and use *packed padded sequences*, which will make our RNN only process the non-padded elements of our sequence, and for any padded element the `output` will be a zero tensor. More info on this [here](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch). \n",
    "\n",
    "To use packed padded sequences, we have to tell the RNN how long the actual sequences are. We do this by setting `include_lengths = True` for our `TEXT` field. This will cause `batch.text` to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "1hMtwr4HzIcb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "# setting the seed so our random output is actually deterministic\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# defining our input fields (text) and labels. \n",
    "# We use the Spacy function because it provides strong support for tokenization in languages other than English\n",
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "bi7KbImtzIcd"
   },
   "source": [
    "We then load the IMDb dataset, which is included in the torchtext package. You can find more information about the built in datasets in torchtext here: https://torchtext.readthedocs.io/en/latest/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zU6tr-XozIce",
    "outputId": "490fa70d-f0a8-4157-a331-649ef0970406"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|| 84.1M/84.1M [00:03<00:00, 24.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "mY38oTI1zIcg"
   },
   "source": [
    "Then we create the validation set from our training set. We will use this to tweek our parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "ghdWKis6zIcg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "XxheAkkc0xWK"
   },
   "source": [
    "What does this dataset looks like? Datasets from torchtext are iterables, so let's get one example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "QJWkpHU50wQ2"
   },
   "outputs": [],
   "source": [
    "example = next(iter(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "AJ_ETyN4ffAl"
   },
   "source": [
    "This contains the text, tokenized (meaning each word is an element of a list): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GazHwGiMfK6C",
    "outputId": "01685304-29eb-4cae-f59b-87fb478946dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Set',\n",
       " 'in',\n",
       " 'the',\n",
       " '1970s',\n",
       " 'Los',\n",
       " 'Angeles',\n",
       " ',',\n",
       " 'Christopher',\n",
       " 'Boyce',\n",
       " 'has',\n",
       " 'just',\n",
       " 'dropped',\n",
       " 'out',\n",
       " 'of',\n",
       " 'seminary',\n",
       " 'school',\n",
       " 'and',\n",
       " 'returned',\n",
       " 'back',\n",
       " 'home',\n",
       " 'were',\n",
       " 'his',\n",
       " 'father',\n",
       " 'gets',\n",
       " 'him',\n",
       " 'a',\n",
       " 'job',\n",
       " 'where',\n",
       " 'he',\n",
       " 'monitors',\n",
       " 'intelligence',\n",
       " 'documents',\n",
       " '.',\n",
       " 'His',\n",
       " 'old',\n",
       " 'friend',\n",
       " 'Daulton',\n",
       " 'Lee',\n",
       " 'is',\n",
       " 'a',\n",
       " 'ratty',\n",
       " 'cock',\n",
       " 'drug',\n",
       " '-',\n",
       " 'dealer',\n",
       " ',',\n",
       " 'and',\n",
       " 'gets',\n",
       " 'caught',\n",
       " 'in',\n",
       " 'a',\n",
       " 'set',\n",
       " '-',\n",
       " 'up',\n",
       " 'and',\n",
       " 'must',\n",
       " 'choose',\n",
       " 'between',\n",
       " 'becoming',\n",
       " 'a',\n",
       " 'narc',\n",
       " 'or',\n",
       " 'facing',\n",
       " 'a',\n",
       " 'long',\n",
       " 'stint',\n",
       " 'in',\n",
       " 'prison',\n",
       " '.',\n",
       " 'When',\n",
       " 'up',\n",
       " 'on',\n",
       " 'bail',\n",
       " ',',\n",
       " 'he',\n",
       " 'jumps',\n",
       " 'and',\n",
       " 'heads',\n",
       " 'to',\n",
       " 'Mexico',\n",
       " 'City',\n",
       " '.',\n",
       " 'Chris',\n",
       " 'offers',\n",
       " 'Lee',\n",
       " 'in',\n",
       " 'a',\n",
       " 'partnership',\n",
       " 'to',\n",
       " 'be',\n",
       " 'his',\n",
       " 'messenger',\n",
       " 'to',\n",
       " 'sell',\n",
       " 'secret',\n",
       " 'papers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Soviet',\n",
       " 'Union',\n",
       " 'embassy',\n",
       " 'in',\n",
       " 'Mexico',\n",
       " 'City',\n",
       " ',',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'disgrace',\n",
       " 'he',\n",
       " 'feels',\n",
       " 'about',\n",
       " 'the',\n",
       " 'US',\n",
       " 'Government',\n",
       " \"'s\",\n",
       " 'control',\n",
       " 'over',\n",
       " 'weaker',\n",
       " 'countries',\n",
       " 'to',\n",
       " 'their',\n",
       " 'own',\n",
       " 'gain',\n",
       " '.',\n",
       " 'But',\n",
       " 'over',\n",
       " 'time',\n",
       " 'the',\n",
       " 'two',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'clash',\n",
       " 'with',\n",
       " 'their',\n",
       " 'motivations',\n",
       " 'and',\n",
       " 'find',\n",
       " 'themselves',\n",
       " 'in',\n",
       " 'something',\n",
       " 'bigger',\n",
       " 'then',\n",
       " 'they',\n",
       " 'had',\n",
       " 'originally',\n",
       " 'intended.<br',\n",
       " '/><br',\n",
       " '/>Director',\n",
       " 'John',\n",
       " 'Schlesinger',\n",
       " 'has',\n",
       " 'spun',\n",
       " 'out',\n",
       " 'such',\n",
       " 'films',\n",
       " 'like',\n",
       " 'the',\n",
       " 'respectable',\n",
       " '\"',\n",
       " 'Midnight',\n",
       " 'Cowboy',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'Marathon',\n",
       " 'Man',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'Sunday',\n",
       " 'Bloody',\n",
       " 'Sunday',\n",
       " '\"',\n",
       " 'and',\n",
       " '\"',\n",
       " 'Day',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Locust',\n",
       " '\"',\n",
       " '.',\n",
       " 'While',\n",
       " '\"',\n",
       " 'The',\n",
       " 'Falcon',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Snowman',\n",
       " '\"',\n",
       " 'might',\n",
       " 'not',\n",
       " 'be',\n",
       " 'held',\n",
       " 'up',\n",
       " 'that',\n",
       " 'high',\n",
       " ',',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'no',\n",
       " 'question',\n",
       " 'that',\n",
       " 'this',\n",
       " 'sombre',\n",
       " 'espionage',\n",
       " 'drama',\n",
       " '(',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'a',\n",
       " 'true',\n",
       " 'incident',\n",
       " ')',\n",
       " 'is',\n",
       " 'an',\n",
       " 'unjustly',\n",
       " 'overlooked',\n",
       " 'character',\n",
       " 'portrait',\n",
       " '.',\n",
       " 'Everything',\n",
       " 'about',\n",
       " 'it',\n",
       " ',',\n",
       " 'is',\n",
       " 'quite',\n",
       " 'a',\n",
       " 'subdued',\n",
       " 'affair',\n",
       " 'with',\n",
       " 'no',\n",
       " 'real',\n",
       " 'grandeur',\n",
       " 'qualities',\n",
       " 'hitting',\n",
       " 'a',\n",
       " 'massive',\n",
       " 'mark',\n",
       " '.',\n",
       " 'The',\n",
       " 'driving',\n",
       " 'factor',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'has',\n",
       " 'got',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'admirably',\n",
       " 'versatile',\n",
       " 'lead',\n",
       " 'performances',\n",
       " 'of',\n",
       " 'Timothy',\n",
       " 'Hutton',\n",
       " 'and',\n",
       " 'Sean',\n",
       " 'Penn',\n",
       " 'as',\n",
       " 'the',\n",
       " 'two',\n",
       " 'ambitious',\n",
       " 'young',\n",
       " 'lads',\n",
       " 'Chris',\n",
       " 'and',\n",
       " 'Daulton',\n",
       " '.',\n",
       " 'Penn',\n",
       " 'is',\n",
       " 'especially',\n",
       " 'good',\n",
       " 'with',\n",
       " 'his',\n",
       " 'uneasy',\n",
       " 'intensity',\n",
       " ',',\n",
       " 'which',\n",
       " 'works',\n",
       " 'well',\n",
       " 'off',\n",
       " 'Hutton',\n",
       " \"'s\",\n",
       " 'superbly',\n",
       " 'cool',\n",
       " '-',\n",
       " 'and',\n",
       " '-',\n",
       " 'collected',\n",
       " 'turn',\n",
       " '.',\n",
       " 'What',\n",
       " 'starts',\n",
       " 'off',\n",
       " 'as',\n",
       " 'easy',\n",
       " ',',\n",
       " 'we',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'gradually',\n",
       " 'crumble',\n",
       " ',',\n",
       " 'as',\n",
       " 'the',\n",
       " 'two',\n",
       " 'amateurs',\n",
       " 'find',\n",
       " 'themselves',\n",
       " 'really',\n",
       " 'out',\n",
       " 'of',\n",
       " 'their',\n",
       " 'league',\n",
       " '.',\n",
       " 'The',\n",
       " 'strongly',\n",
       " 'detailed',\n",
       " 'and',\n",
       " 'symbolic',\n",
       " '(',\n",
       " 'predatory',\n",
       " 'behaviour',\n",
       " ')',\n",
       " 'plot',\n",
       " 'mainly',\n",
       " 'centres',\n",
       " 'on',\n",
       " 'the',\n",
       " 'pair',\n",
       " \"'s\",\n",
       " 'relationship',\n",
       " 'and',\n",
       " 'that',\n",
       " 'of',\n",
       " 'their',\n",
       " 'reasoning',\n",
       " \"'s\",\n",
       " 'for',\n",
       " 'their',\n",
       " 'actions',\n",
       " ',',\n",
       " 'which',\n",
       " 'eventually',\n",
       " 'shows',\n",
       " 'us',\n",
       " 'the',\n",
       " 'knotty',\n",
       " 'developments',\n",
       " 'that',\n",
       " 'led',\n",
       " 'to',\n",
       " 'their',\n",
       " 'downfall',\n",
       " '.',\n",
       " 'The',\n",
       " 'plan',\n",
       " 'opens',\n",
       " 'up',\n",
       " 'like',\n",
       " 'a',\n",
       " 'wound',\n",
       " 'to',\n",
       " 'never',\n",
       " 'properly',\n",
       " 'heal',\n",
       " ',',\n",
       " 'due',\n",
       " 'to',\n",
       " 'Daulton',\n",
       " \"'s\",\n",
       " 'drug',\n",
       " 'addiction',\n",
       " ',',\n",
       " 'which',\n",
       " 'really',\n",
       " 'makes',\n",
       " 'him',\n",
       " 'go',\n",
       " 'off',\n",
       " 'the',\n",
       " 'rails',\n",
       " 'and',\n",
       " 'leaves',\n",
       " 'Chris',\n",
       " 'to',\n",
       " 'pick',\n",
       " 'up',\n",
       " 'all',\n",
       " 'the',\n",
       " 'slack',\n",
       " '.',\n",
       " 'The',\n",
       " 'searing',\n",
       " 'political',\n",
       " 'aspect',\n",
       " 'is',\n",
       " 'there',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'the',\n",
       " 'themes',\n",
       " 'of',\n",
       " 'idealism',\n",
       " '(',\n",
       " 'Boyce',\n",
       " ')',\n",
       " 'and',\n",
       " 'greed',\n",
       " '(',\n",
       " 'Lee',\n",
       " ')',\n",
       " 'to',\n",
       " 'get',\n",
       " 'its',\n",
       " 'point',\n",
       " 'across',\n",
       " '.',\n",
       " 'Both',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mix',\n",
       " 'and',\n",
       " 'results',\n",
       " 'show',\n",
       " '.',\n",
       " 'Suspense',\n",
       " 'is',\n",
       " 'justified',\n",
       " 'through',\n",
       " 'its',\n",
       " 'stimulating',\n",
       " 'pot',\n",
       " '-',\n",
       " 'boiling',\n",
       " 'script',\n",
       " 'and',\n",
       " 'character',\n",
       " 'interactions',\n",
       " 'then',\n",
       " 'that',\n",
       " 'of',\n",
       " 'any',\n",
       " 'visual',\n",
       " 'gimmicks',\n",
       " '.',\n",
       " 'Action',\n",
       " 'is',\n",
       " 'very',\n",
       " 'little',\n",
       " ',',\n",
       " 'but',\n",
       " 'still',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'pressure',\n",
       " 'induced',\n",
       " 'style',\n",
       " 'to',\n",
       " 'Schlesinger',\n",
       " \"'s\",\n",
       " 'assured',\n",
       " 'and',\n",
       " 'realistically',\n",
       " 'dark',\n",
       " \"'\",\n",
       " 'n',\n",
       " \"'\",\n",
       " 'gritty',\n",
       " 'direction',\n",
       " '.',\n",
       " 'Pacing',\n",
       " 'is',\n",
       " 'mostly',\n",
       " 'well',\n",
       " 'handled',\n",
       " ',',\n",
       " 'although',\n",
       " 'some',\n",
       " 'sequences',\n",
       " 'do',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'wallow',\n",
       " 'on',\n",
       " 'for',\n",
       " 'too',\n",
       " 'long',\n",
       " ',',\n",
       " 'but',\n",
       " 'however',\n",
       " 'it',\n",
       " 'grips',\n",
       " 'you',\n",
       " 'as',\n",
       " 'it',\n",
       " 'plays',\n",
       " 'on',\n",
       " 'its',\n",
       " 'authentically',\n",
       " 'paranoid',\n",
       " 'tone',\n",
       " 'to',\n",
       " 'slowly',\n",
       " 'build',\n",
       " 'up',\n",
       " 'to',\n",
       " 'an',\n",
       " 'exploding',\n",
       " 'tight',\n",
       " 'latter',\n",
       " 'end',\n",
       " '.',\n",
       " 'Adeptly',\n",
       " 'fleshed',\n",
       " 'into',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'production',\n",
       " 'is',\n",
       " 'an',\n",
       " 'airily',\n",
       " 'harrowing',\n",
       " 'music',\n",
       " 'score',\n",
       " 'and',\n",
       " 'professionally',\n",
       " 'poignant',\n",
       " 'cinematography',\n",
       " '.',\n",
       " 'The',\n",
       " 'supporting',\n",
       " 'cast',\n",
       " 'are',\n",
       " 'exceptionally',\n",
       " 'fine',\n",
       " 'with',\n",
       " 'Pat',\n",
       " 'Hingle',\n",
       " ',',\n",
       " 'Lori',\n",
       " 'Singer',\n",
       " ',',\n",
       " 'David',\n",
       " 'Suchet',\n",
       " ',',\n",
       " 'Boris',\n",
       " 'Leskin',\n",
       " ',',\n",
       " 'Jerry',\n",
       " 'Hardin',\n",
       " 'and',\n",
       " 'Joyce',\n",
       " 'Van',\n",
       " 'Patten',\n",
       " '.',\n",
       " 'Also',\n",
       " 'look',\n",
       " 'out',\n",
       " 'for',\n",
       " 'Michael',\n",
       " 'Ironside',\n",
       " 'in',\n",
       " 'a',\n",
       " 'tiny',\n",
       " 'part',\n",
       " 'as',\n",
       " 'a',\n",
       " 'FBI',\n",
       " 'agent.<br',\n",
       " '/><br',\n",
       " '/>A',\n",
       " 'mostly',\n",
       " 'outstanding',\n",
       " 'spy',\n",
       " '-',\n",
       " 'film',\n",
       " 'that',\n",
       " 'benefits',\n",
       " 'largely',\n",
       " 'from',\n",
       " 'talented',\n",
       " 'lead',\n",
       " 'performances',\n",
       " 'and',\n",
       " 'by',\n",
       " 'not',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'stakes',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'more',\n",
       " 'an',\n",
       " 'emotional',\n",
       " 'ride',\n",
       " ',',\n",
       " 'then',\n",
       " 'a',\n",
       " 'complex',\n",
       " 'one',\n",
       " 'of',\n",
       " 'twists',\n",
       " '.',\n",
       " 'Recommended',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ExnCED8LfmyE"
   },
   "source": [
    "And a label (in this case 'pos' for positive sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TB4XMzhUfm6R",
    "outputId": "9c3bebce-de01-4c75-e033-49663eb3d546"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "dE4kUQ11zIci"
   },
   "source": [
    "Next comes loading the pre-trained word embeddings. Now, instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors, namely those from glove (Stanford). There are other alternatives you can use here. \n",
    "\n",
    "We get these vectors simply by specifying which vectors we want and passing it as an argument to `build_vocab`. `TorchText` handles downloading the vectors and associating them with the correct words in our vocabulary.\n",
    "\n",
    "Here, we'll be using the `\"glove.6B.100d\" vectors\"`. `glove` is the algorithm used to calculate the vectors, go [here](https://nlp.stanford.edu/projects/glove/) for more. `6B` indicates these vectors were trained on 6 billion tokens and `100d` indicates these vectors are 100-dimensional, meaning there are 100 values (or coordinates) for each word.\n",
    "\n",
    "You can see the other available vectors [here](https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L113).\n",
    "\n",
    "The theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch.\n",
    "\n",
    "**Note**: these vectors are about 862MB.\n",
    "\n",
    "By default, TorchText will initialize words in your vocabulary but not in your pre-trained embeddings (so in order words: new words), to zero. We don't want this, and instead initialize them randomly by setting `unk_init` to `torch.Tensor.normal_`. This will now initialize those words via a Gaussian distribution.\n",
    "\n",
    "We limit the number of unique words we use to 25,000. The least frequent words will be replaced by the word `unk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vt2NxMxvzIcj",
    "outputId": "a6ee63a6-cc7d-400b-c294-1e13b272afd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:29, 2.22MB/s]\n",
      "100%|| 398274/400000 [00:15<00:00, 25860.93it/s]"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_) # how to initialize unseen words not in glove\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "BhZEuYcKzIcl"
   },
   "source": [
    "We create the iterators, placing the tensors on the GPU if one is available.\n",
    "\n",
    "one consderation for packed padded sequences all of the tensors within a batch need to be sorted by their lengths. This is handled in the batch iterator by setting `sort_within_batch = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "433TeUABzIcl"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4RX_M6P3zIcn"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "Alright, we have our data pipeline set up and are ready to define the model. \n",
    "\n",
    "### Different RNN Architecture\n",
    "\n",
    "We'll be using the RNN architecture called a Long Short-Term Memory (LSTM). Why is an LSTM better than a standard RNN? Standard RNNs suffer from the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). LSTMs overcome this by having an extra recurrent state called a _cell_, $c$ - which can be thought of as the \"memory\" of the LSTM - and the use use multiple _gates_ which control the flow of information into and out of the memory. For more information, go [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n",
    "\n",
    "$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n",
    "\n",
    "Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment2.png)\n",
    "\n",
    "The initial cell state, $c_0$, like the initial hidden state is initialized to a tensor of all zeros. The sentiment prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=f(h_T)$.\n",
    "\n",
    "\n",
    "### Bidirectional RNN\n",
    "\n",
    "In this lab we are going one step above the traditional LSTM, by using a **bidirectional** one, which may be even more effective. \n",
    "\n",
    "The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n",
    "\n",
    "In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor. \n",
    "\n",
    "We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$   \n",
    "\n",
    "The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.  \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment3.png)\n",
    "\n",
    "### Multi-layer RNN\n",
    "\n",
    "Multi-layer RNNs (also called *deep RNNs*) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another *layer*. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n",
    "\n",
    "The image below shows a multi-layer unidirectional RNN, where the layer number is given as a superscript. Also note that each layer needs their own initial hidden state, $h_0^L$.\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment4.png)\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Although we've added improvements to our model, each one adds additional parameters. Without going into overfitting into too much detail, the more parameters you have in in your model, the higher the probability that your model will overfit (memorize the training data, causing  a low training error but high validation/testing error, i.e. poor generalization to new, unseen examples). To combat this, we use regularization. More specifically, we use a method of regularization called *dropout*. Dropout works by randomly *dropping out* (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently. One theory about why dropout works is that a model with parameters dropped out can be seen as a \"weaker\" (less parameters) model. The predictions from all these \"weaker\" models (one for each forward pass) get averaged together withinin the parameters of the model. Thus, your one model can be thought of as an ensemble of weaker models, none of which are over-parameterized and thus should not overfit.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "Another addition to this model is that we are not going to learn the embedding for the `<pad>` token. This is because we want to explitictly tell our model that padding tokens are irrelevant to determining the sentiment of a sentence. This means the embedding for the pad token will remain at what it is initialized to (we initialize it to all zeros later). We do this by passing the index of our pad token as the `padding_idx` argument to the `nn.Embedding` layer.\n",
    "\n",
    "To use an LSTM instead of the standard RNN, we use `nn.LSTM` instead of `nn.RNN`. Also, note that the LSTM returns the `output` and a tuple of the final `hidden` state and the final `cell` state, whereas the standard RNN only returned the `output` and final `hidden` state. \n",
    "\n",
    "As the final hidden state of our LSTM has both a forward and a backward component, which will be concatenated together, the size of the input to the `nn.Linear` layer is twice that of the hidden dimension size.\n",
    "\n",
    "Implementing bidirectionality and adding additional layers are done by passing values for the `num_layers` and `bidirectional` arguments for the RNN/LSTM. \n",
    "\n",
    "Dropout is implemented by initializing an `nn.Dropout` layer (the argument is the probability of dropping out each neuron) and using it within the `forward` method after each layer we want to apply dropout to. **Note**: never use dropout on the input or output layers (`text` or `fc` in this case), you only ever want to use dropout on intermediate layers. The LSTM has a `dropout` argument which adds dropout on the connections between hidden states in one layer to hidden states in the next layer. \n",
    "\n",
    "As we are passing the lengths of our sentences to be able to use packed padded sequences, we have to add a second argument, `text_lengths`, to `forward`. \n",
    "\n",
    "Before we pass our embeddings to the RNN, we need to pack them, which we do with `nn.utils.rnn.packed_padded_sequence`. This will cause our RNN to only process the non-padded elements of our sequence. The RNN will then return `packed_output` (a packed sequence) as well as the `hidden` and `cell` states (both of which are tensors). Without packed padded sequences, `hidden` and `cell` are tensors from the last element in the sequence, which will most probably be a pad token, however when using packed padded sequences they are both from the last non-padded element in the sequence. \n",
    "\n",
    "We then unpack the output sequence, with `nn.utils.rnn.pad_packed_sequence`, to transform it from a packed sequence to a tensor. The elements of `output` from padding tokens will be zero tensors (tensors where every element is zero). Usually, we only have to unpack output if we are going to use it later on in the model. Although we aren't in this case, we still unpack the sequence just to show how it is done.\n",
    "\n",
    "The final hidden state, `hidden`, has a shape of _**[num layers * num directions, batch size, hid dim]**_. These are ordered: **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**. As we want the final (top) layer forward and backward hidden states, we get the top two hidden layers from the first dimension, `hidden[-2,:,:]` and `hidden[-1,:,:]`, and concatenate them together before passing them to the linear layer (after applying dropout). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "GwBWdeEszIco"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, #'100'\n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, #set to two: makes our LSTM 'deep'\n",
    "                           bidirectional=bidirectional, #bidirectional or not\n",
    "                           dropout=dropout) #we add dropout for regularization\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) #dropout layer\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text)) ## change the text to the embedding\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths) #use packed padding\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded) #feed to rnn\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output) #unpack the padding\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) #add dropout\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "LtxnuD_zzIcq"
   },
   "source": [
    "Like before, we'll create an instance of our RNN class, with the new parameters and arguments for the number of layers, bidirectionality and dropout probability.\n",
    "\n",
    "To ensure the pre-trained vectors can be loaded into the model, the `EMBEDDING_DIM` must be equal to that of the pre-trained GloVe vectors loaded earlier, which is 100 if you recall from above. \n",
    "\n",
    "We get our pad token index from the vocabulary, getting the actual string representing the pad token from the field's `pad_token` attribute, which is `<pad>` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "y9Y5TMKyzIcq"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2 #this makes our LSTM deep\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # this will be passed to the lstm, to tell it when id the word is in our vocabulary\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "mC8njeTezIct"
   },
   "source": [
    "We'll print out the number of parameters in our model. \n",
    "\n",
    "Notice how we have almost twice as many parameters as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WXFQ03s5zIct",
    "outputId": "ff480852-d615-4c5a-8333-0f52aaabe29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "NQlJj6QXzIcw"
   },
   "source": [
    "The final addition is copying the pre-trained word embeddings we loaded earlier into the `embedding` layer of our model.\n",
    "\n",
    "We retrieve the embeddings from the field's vocab, and check they're the correct size, _**[vocab size, embedding dim]**_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3ITV4jjuzIcw",
    "outputId": "2e8e8158-085d-477c-a240-cfb79d473025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "pqMdc3yhzIcy"
   },
   "source": [
    "We then replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n",
    "\n",
    "**Note**: this should always be done on the `weight.data` and not the `weight`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "jqHqNiITzIcy",
    "outputId": "3e6a9776-4b44-4463-e954-3c2a3db5911a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.0046, -0.4325, -0.3023,  ...,  0.3291, -0.1993, -0.2312],\n",
       "        [-0.6806,  0.4531, -0.0683,  ..., -0.0388,  0.4975, -0.0208],\n",
       "        [ 0.1619,  0.4196, -0.0119,  ..., -0.2165, -0.2771,  0.1419]])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "fibaVxJIzIc1"
   },
   "source": [
    "As our `<unk>` and `<pad>` token aren't in the pre-trained vocabulary they have been initialized using `unk_init` (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment. \n",
    "\n",
    "We do this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens, which we have already done for the padding index.\n",
    "\n",
    "**Note**: like initializing the embeddings, this should be done on the `weight.data` and not the `weight`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "X3Vih_EzzIc1",
    "outputId": "51c859f7-65fc-4bf3-f5ef-4d779ff02515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.0046, -0.4325, -0.3023,  ...,  0.3291, -0.1993, -0.2312],\n",
      "        [-0.6806,  0.4531, -0.0683,  ..., -0.0388,  0.4975, -0.0208],\n",
      "        [ 0.1619,  0.4196, -0.0119,  ..., -0.2165, -0.2771,  0.1419]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ffx2rQFKzIc3"
   },
   "source": [
    "We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "IwwwGmXizIc3"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "cQ5BjKErzIc4"
   },
   "source": [
    "Now to training the model.\n",
    "\n",
    "The only change we'll make here is changing the optimizer from `SGD` to `Adam`. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. More information about `Adam` (and other optimizers) can be found [here](http://ruder.io/optimizing-gradient-descent/index.html).\n",
    "\n",
    "To change `SGD` to `Adam`, we simply change `optim.SGD` to `optim.Adam`, also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "82djifYvzIc4"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "FszhCJo3zIc6"
   },
   "source": [
    "The rest of the steps for training the model are unchanged.\n",
    "\n",
    "We define the criterion and place the model and criterion on the GPU (if available)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "r7jJZVUrzIc6"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "1-AmguGBzIc8"
   },
   "source": [
    "We implement the function to calculate accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "2VSigFwuzIc8"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ruL3QL0yzIc-"
   },
   "source": [
    "We define a function for training our model. \n",
    "\n",
    "As we have set `include_lengths = True`, our `batch.text` is now a tuple with the first element being the numericalized tensor and the second element being the actual lengths of each sequence. We separate these into their own variables, `text` and `text_lengths`, before passing them to the model.\n",
    "\n",
    "**Note**: as we are now using dropout, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "YVjJn_1CzIc-"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "UV1VKsb8zIdA"
   },
   "source": [
    "Then we define a function for testing our model, again remembering to separate `batch.text`.\n",
    "\n",
    "**Note**: as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "374ehVXpzIdB"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "NnwdBd7EzIdC"
   },
   "source": [
    "And also create a nice function to tell us how long our epochs are taking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "9rbX-lwyzIdD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "QQ5TSRpTzIdE"
   },
   "source": [
    "Finally, we train our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "wrxt624SzIdE",
    "outputId": "53e28c7c-8bbf-446e-b984-4834f926d216"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 398274/400000 [00:29<00:00, 25860.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 57s\n",
      "\tTrain Loss: 0.655 | Train Acc: 60.18%\n",
      "\t Val. Loss: 0.572 |  Val. Acc: 70.24%\n",
      "Epoch: 02 | Epoch Time: 0m 57s\n",
      "\tTrain Loss: 0.583 | Train Acc: 68.29%\n",
      "\t Val. Loss: 0.418 |  Val. Acc: 81.17%\n",
      "Epoch: 03 | Epoch Time: 0m 57s\n",
      "\tTrain Loss: 0.388 | Train Acc: 83.21%\n",
      "\t Val. Loss: 0.331 |  Val. Acc: 85.13%\n",
      "Epoch: 04 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.325 | Train Acc: 86.34%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 87.66%\n",
      "Epoch: 05 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.272 | Train Acc: 89.03%\n",
      "\t Val. Loss: 0.285 |  Val. Acc: 88.45%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "x7_QToA4zIdG"
   },
   "source": [
    "...and get our new and vastly improved test accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "voYDzXhszIdG",
    "outputId": "62fc21c4-c16d-402c-dd2c-1963e04c8839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.291 | Test Acc: 88.08%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "yipJVKzhzIdI"
   },
   "source": [
    "## User Input\n",
    "\n",
    "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n",
    "\n",
    "When using a model for inference it should always be in evaluation mode. If this tutorial is followed step-by-step then it should already be in evaluation mode (from doing `evaluate` on the test set), however we explicitly set it to avoid any risk.\n",
    "\n",
    "Our `predict_sentiment` function does a few things:\n",
    "- sets the model to evaluation mode\n",
    "- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
    "- indexes the tokens by converting them into their integer representation from our vocabulary\n",
    "- gets the length of our sequence\n",
    "- converts the indexes, which are a Python list into a PyTorch tensor\n",
    "- add a batch dimension by `unsqueeze`ing \n",
    "- converts the length into a tensor\n",
    "- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
    "- converts the tensor holding a single value into an integer with the `item()` method\n",
    "\n",
    "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "gNQ6_af5zIdI"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "0qfIehpazIdK"
   },
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CKHqv3tQzIdK",
    "outputId": "20fc57b1-1181-43f7-9d97-7aa62aa1f6a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009414942003786564"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "bpYmjWQizIdN"
   },
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o3OdPELdzIdN",
    "outputId": "2c1e5b8c-bafd-4faa-9907-1e123a69159d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797689318656921"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "6C4np6TBEToZ"
   ],
   "name": "AI class - PyTorch Neural Networks",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "063343a6ed0c4430a59ed2a43adeced7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3619f56399b04a5cae084253d490cb24",
       "IPY_MODEL_6178f6d241d24e5bb80d0849592dc46f"
      ],
      "layout": "IPY_MODEL_b7ad32af1be54da8b3a9bcb87a6f46c9"
     }
    },
    "0af4442d59474db3bdf48b8d43c5630c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30b7d427ca524d5bbb540f9a29f925d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33972cfe3dba42f7b7cb22d1bb6f2c35": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33f292e3ed83456fb25310adfdd51eed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a30628017c8c49b193fd128edd92fb16",
      "placeholder": "",
      "style": "IPY_MODEL_3e283adad2754727a1a542d6c2cc3963",
      "value": " 8192/? [00:00&lt;00:00, 14898.27it/s]"
     }
    },
    "3619f56399b04a5cae084253d490cb24": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0af4442d59474db3bdf48b8d43c5630c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_861879c943bd4adf8fa20cd29d6574a6",
      "value": 1
     }
    },
    "3b87ae347817466abae90da93bb18cc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9df8d475e6b4541a04d1fcaadd21cea",
       "IPY_MODEL_60ec53e0872345a993ebfe93a79eb6ef"
      ],
      "layout": "IPY_MODEL_4612536ae2bc405789cd111b81095c20"
     }
    },
    "3e283adad2754727a1a542d6c2cc3963": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41f12ed008d44ccfab604edf77ce97fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54a601dbf01e44dc9243d8dddf293626",
      "placeholder": "",
      "style": "IPY_MODEL_5c849e15cb674970ba757ee7ef7f2d34",
      "value": " 26427392/? [00:20&lt;00:00, 10823064.50it/s]"
     }
    },
    "4612536ae2bc405789cd111b81095c20": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51c9865f5c364560861542eca2a0fb1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "51e80c76578c400ab4d197ea6344c437": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54a601dbf01e44dc9243d8dddf293626": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c849e15cb674970ba757ee7ef7f2d34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60ec53e0872345a993ebfe93a79eb6ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33972cfe3dba42f7b7cb22d1bb6f2c35",
      "placeholder": "",
      "style": "IPY_MODEL_61817e09ee6644409c6f94c437c09085",
      "value": " 32768/? [00:17&lt;00:00, 153190.63it/s]"
     }
    },
    "6178f6d241d24e5bb80d0849592dc46f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8925827917604b13bbe5739e48b064e6",
      "placeholder": "",
      "style": "IPY_MODEL_30b7d427ca524d5bbb540f9a29f925d2",
      "value": " 4423680/? [00:01&lt;00:00, 2499848.25it/s]"
     }
    },
    "61817e09ee6644409c6f94c437c09085": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b190e8d15914abb83564c6838659743": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "730a7fcd528a433b8dfb0ec76d09db28": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "861879c943bd4adf8fa20cd29d6574a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8925827917604b13bbe5739e48b064e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92a296dbe5ba4293872f777ac62e279e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "946e06cfaa6c44218f7f2a9800587325": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3b8b5473461477a9d6c838029137c4a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f649c6038d9d4a6d9c60f66b11ca51bc",
      "value": 1
     }
    },
    "a30628017c8c49b193fd128edd92fb16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7b0091ca99b4a0493855c50a5f6d3b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51e80c76578c400ab4d197ea6344c437",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9626a1df05741a0b8929bc590b4f310",
      "value": 1
     }
    },
    "a9626a1df05741a0b8929bc590b4f310": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b7ad32af1be54da8b3a9bcb87a6f46c9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9df8d475e6b4541a04d1fcaadd21cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b190e8d15914abb83564c6838659743",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51c9865f5c364560861542eca2a0fb1a",
      "value": 1
     }
    },
    "c3b8b5473461477a9d6c838029137c4a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4fdec40a7a84c4aa22d7a6aadc6f0b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_946e06cfaa6c44218f7f2a9800587325",
       "IPY_MODEL_33f292e3ed83456fb25310adfdd51eed"
      ],
      "layout": "IPY_MODEL_92a296dbe5ba4293872f777ac62e279e"
     }
    },
    "e786c2ca602c495d9a5cef9f8c156f9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a7b0091ca99b4a0493855c50a5f6d3b1",
       "IPY_MODEL_41f12ed008d44ccfab604edf77ce97fd"
      ],
      "layout": "IPY_MODEL_730a7fcd528a433b8dfb0ec76d09db28"
     }
    },
    "f649c6038d9d4a6d9c60f66b11ca51bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
